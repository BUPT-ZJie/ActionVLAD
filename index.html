
<!-- Taken from url=http://www.cs.cmu.edu/~dfouhey/3DP/index.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="StyleSheet" href="assets/style.css" type="text/css" media="all">

<title>ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
<script type="text/javascript" async="" src="assets/ga.js"></script><script type="text/javascript">
</script>

<!-- bibliographic tags -->
<meta name="citation_title" content="ActionVLAD: Learning spatio-temporal aggregation for action classification"/>
<meta name="citation_author" content="Girdhar, Rohit"/>
<meta name="citation_author" content="Ramanan, Deva"/>
<meta name="citation_author" content="Gupta, Abhinav"/>
<meta name="citation_author" content="Sivic, Josef"/>
<meta name="citation_author" content="Russell, Bryan"/>
<meta name="citation_publication_date" content="2017"/>
<meta name="citation_journal_title" content=""/>
<meta name="citation_pdf_url" content=""/>

<style type="text/css">
#primarycontent h1 {
	font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
	text-align: center;
}
#primarycontent p {
	text-align: center;
}
#primarycontent {
	text-align: justify;
}
#primarycontent p {
	text-align: justify;
}
#primarycontent p iframe {
	text-align: center;
}
</style>
<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>

<link rel="icon" type="image/png" href="http://rohitgirdhar.github.io/favicon.png">

</head>
<body>
<div id="primarycontent">
<h1 align="center" itemprop="name"><strong>
ActionVLAD: Learning spatio-temporal <br/> aggregation for action classification
</strong></h1>


   <table class="results" align="center">
    <tr>
      <td align="center">
	      <img src="assets/teaser.png" width="70%" /></a>
      </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>
    <tr>
      <td class="credits" align="justify">
        In this work, we introduce a new video representation for action classification that
        aggregates local convolutional features across the entire spatio-temporal extent of the video.
        We do so by integrating state-of-the-art two-stream networks with learnable spatio-temporal NetVLAD codebooks.
        The resulting architecture is end-to-end trainable for whole-video classification.
        We investigate different strategies for pooling across space and time and combining signals from
        the different streams. We find that  (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations.
        Finally,  we  show  that  our representation  outperforms  the  two-stream  base  architecture by a large margin (13% relative) as well as outperforms other baselines with comparable base architectures when combined with iDT, on HMDB51 and UCF101 video classification benchmarks.
	  </td>
    </trt>
 </table>



<h3>People</h3>

<ul id="people">
<li><a href="http://www.cs.cmu.edu/~rgirdhar/">Rohit Girdhar</a></li>
<li><a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan</a></li>
<li><a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a></li>
<li><a href="http://www.di.ens.fr/~josef/">Josef Sivic</a></li>
<li><a href="http://www.bryanrussell.org">Bryan Russell</a></li>
</ul>


<h3>Paper</h3>
<table>
  <tr></tr>
  <tr><td>
    <a href="docs/paper.pdf"><img style="box-shadow: 5px 5px 2px #888888; margin: 10px" src="assets/paper-screenshot.png" width="150px"/></a>
  </td>
  <td></td>
  <td>
    R. Girdhar, D. Ramanan, A. Gupta, J. Sivic and B. Russell <br/>
    <a href="https://rohitgirdhar.github.io/ActionVLAD">ActionVLAD: Learning spatio-temporal
    aggregation for action classification</a> <br/>
    In Proc. of Conference on <i>Computer Vision and Pattern Recognition (CVPR)</i>, 2017 <br/>
    [<a href="docs/#">PDF</a>] [<a href="">arXiv</a>] [<a href="">code</a>] [<a href="">Supplementary</a>]
    [<a href="javascript:togglevis('girdhar17a')" id="bibtex">BibTex</a>]
</table>



<table class="bibtex" style="display:none" id="girdhar17a"><tr><td>
<pre>
    @inproceedings{Girdhar_17a_ActionVLAD,
        title = {ActionVLAD: Learning spatio-temporal
                 aggregation for action classification},
        author = {Girdhar, Rohit and
                  Ramanan, Deva and
                  Gupta, Abhinav and
                  Sivic, Josef and
                  Russell, Bryan},
        booktitle = {CVPR},
        year = 2017
    }
</pre>
</td></tr></table>

<h3>Acknowledgements</h3>
<p>
This work was partially supported by Siebel Scholarship to
RG, NDSEG Fellowship to DF and Bosch Young Faculty Fellowship to AG. This
material is based on research partially sponsored by ONR MURI N000141010934,
ONR MURI N000141612007, NSF1320083 and a gift from Google. The authors
would like to thank Yahoo! and Nvidia for the compute cluster and GPU donations
respectively. The authors would also like to thank Martial Hebert and
Xiaolong Wang for many helpful discussions.
</div>

</body></html>


