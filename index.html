
<!-- Taken from url=http://www.cs.cmu.edu/~dfouhey/3DP/index.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="StyleSheet" href="assets/style.css" type="text/css" media="all">

<title>ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
<script type="text/javascript" async="" src="assets/ga.js"></script><script type="text/javascript">
</script>

<!-- bibliographic tags -->
<meta name="citation_title" content="ActionVLAD: Learning spatio-temporal aggregation for action classification"/>
<meta name="citation_author" content="Girdhar, Rohit"/>
<meta name="citation_author" content="Ramanan, Deva"/>
<meta name="citation_author" content="Gupta, Abhinav"/>
<meta name="citation_author" content="Sivic, Josef"/>
<meta name="citation_author" content="Russell, Bryan"/>
<meta name="citation_publication_date" content="2017"/>
<meta name="citation_journal_title" content="CVPR"/>
<meta name="citation_pdf_url" content=""/>

<style type="text/css">
#primarycontent h1 {
	font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
	text-align: center;
}
#primarycontent p {
	text-align: center;
}
#primarycontent {
	text-align: justify;
}
#primarycontent p {
	text-align: justify;
}
#primarycontent p iframe {
	text-align: center;
}
</style>
<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>

<link rel="icon" type="image/png" href="http://rohitgirdhar.github.io/favicon.png">

</head>
<body>
<div id="primarycontent">
<h1 align="center" itemprop="name"><strong>
ActionVLAD: Learning spatio-temporal <br/> aggregation for action classification
</strong></h1>


   <table class="results" align="center">
    <tr>
      <td align="center">
	      <img src="assets/teaser.png" width="70%" /></a>
      </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>
    <tr>
      <td class="credits" align="justify">
        In this work, we introduce a new video representation for action classification that
        aggregates local convolutional features across the entire spatio-temporal extent of the video.
        We do so by integrating state-of-the-art two-stream networks with learnable spatio-temporal NetVLAD codebooks.
        The resulting architecture is end-to-end trainable for whole-video classification.
        We investigate different strategies for pooling across space and time and combining signals from
        the different streams. We find that  (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations.
        Finally,  we  show  that  our representation  outperforms  the  two-stream  base  architecture by a large margin (13% relative) as well as outperforms other baselines with comparable base architectures on HMDB51, UCF101 and Charades video classification benchmarks.
	  </td>
    </trt>
 </table>



<h3>People</h3>

<table id="people">
  <tr>
    <td>
      <img src="assets/authors/rohit.jpg"/><br/>
      <a href="http://www.cs.cmu.edu/~rgirdhar/">Rohit Girdhar</a>
    </td>
    <td>
      <img src="assets/authors/deva.jpg"/><br/>
      <a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
    </td>
    <td>
      <img src="assets/authors/abhinav.jpg"/><br/>
      <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>
    </td>
    <td>
      <img src="assets/authors/josef.jpg"/><br/>
      <a href="http://www.di.ens.fr/~josef/">Josef Sivic</a>
    </td>
    <td>
      <img src="assets/authors/bryan.jpg"/><br/>
      <a href="http://www.bryanrussell.org">Bryan Russell</a>
    </td>
  </tr>
</table>


<h3>Paper</h3>
<table>
  <tr></tr>
  <tr><td>
    <a href="docs/paper.pdf"><img style="box-shadow: 5px 5px 2px #888888; margin: 10px" src="assets/paper-screenshot.png" width="150px"/></a>
  </td>
  <td></td>
  <td>
    R. Girdhar, D. Ramanan, A. Gupta, J. Sivic and B. Russell <br/>
    <a href="https://rohitgirdhar.github.io/ActionVLAD">ActionVLAD: Learning spatio-temporal
    aggregation for action classification</a> <br/>
    In Proc. of Conference on <i>Computer Vision and Pattern Recognition (CVPR)</i>, 2017 <br/>
    [<a href="arxiv/">arXiv</a>] [<a href="http://github.com/rohitgirdhar/ActionVLAD/">code/models</a>] [<a href="https://www.youtube.com/watch?v=wVde6BPVUM0">supplementary video</a>]
    [<a href="javascript:togglevis('girdhar17a')" id="bibtex">BibTex</a>]
</table>



<table class="bibtex" style="display:none" id="girdhar17a"><tr><td>
<pre>
    @inproceedings{Girdhar_17a_ActionVLAD,
        title = {{ActionVLAD}: Learning spatio-temporal
                 aggregation for action classification},
        author = {Girdhar, Rohit and
                  Ramanan, Deva and
                  Gupta, Abhinav and
                  Sivic, Josef and
                  Russell, Bryan},
        booktitle = {CVPR},
        year = 2017
    }
</pre>
</td></tr></table>

<h3>Acknowledgements</h3>
<p>
The authors would like to thank Gul Varol and Gunnar Atli Sigurdsson for help with iDT.
DR is supported by NSF Grant 1618903, Google, and the Intel Science and Technology Center for Visual Cloud Systems (ISTC-VCS).
</p>
</div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71557010-3', 'auto');
  ga('send', 'pageview');

</script>

</body></html>


